{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29255070",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"E:\\CHANDRU\\chand proj\\MOUD\\audioFiles\"\n",
    "target_data = pd.read_csv(\"E:\\\\CHANDRU\\\\chand proj\\\\MOUD\\\\target_audio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "#colors_encoded = le.fit_transform(target)\n",
    "target_data['class'] = le.fit_transform(target_data['class'])\n",
    "target_data['class'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    audio, sr = librosa.load(filepath)\n",
    "#     audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "\n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    " \n",
    "    return mfccs_scaled_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFun(file):\n",
    "    audio, sr = librosa.load(filepath)\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n",
    "    S_dB_mel = librosa.amplitude_to_db(S, ref=np.max)\n",
    " \n",
    "    fig,ax = plt.subplots(figsize=(10, 5))\n",
    "    img = librosa.display.specshow(S_dB_mel, y_axis='log', x_axis='time', ax=ax)\n",
    "    ax.set_title('Power spectrogram')\n",
    "    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5428959",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a=target_data[\"instance\"].to_numpy()\n",
    "b=[]\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        filepath = os.path.join(dataset_path, filename)\n",
    "        b.append(filename)\n",
    "ar1=[]\n",
    "for i in b:\n",
    "    st1=i.split(\".\")[0]\n",
    "    ar1.append(st1)\n",
    "ar2=np.array(ar1)\n",
    "\n",
    "d= np.intersect1d(a,ar2)\n",
    "z=np.setxor1d(a,ar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da16f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "### Now we iterate through every audio file and extract features \n",
    "### using Mel-Frequency Cepstral Coefficients\n",
    "extracted_features=[]\n",
    "import numpy as np\n",
    "cnt=0\n",
    "#extracted_features=[]\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        str1=filename.split(\".\")[0]\n",
    "        if str1 in z:\n",
    "            print(str1)\n",
    "        else:\n",
    "            cnt=cnt+1\n",
    "            print(cnt)        \n",
    "        # Load the audio file and resample it to the target sampling rate\n",
    "            filepath = os.path.join(dataset_path, filename)\n",
    "            plotFun(filepath)\n",
    "            data=features_extractor(filepath)\n",
    "        \n",
    "            extracted_features.append([data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71114e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de380acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(target_data[['class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f29bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afcbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "### firstlayer\n",
    "#model.add(Dense(100,input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(100,input_shape=(40,)))\n",
    "\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "### second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "### third layer \n",
    "model.add(Dense(100))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "###final layer\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da913e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",metrics=[\"accuracy\"],optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4da5db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Trianing my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs=80\n",
    "num_batch_size=32\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath=\"best_model1.h5\",verbose=1, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "start=datetime.now()\n",
    "model.fit(X_train,y_train,batch_size=num_batch_size,epochs=num_epochs,validation_data=(X_train,y_train),callbacks=[checkpoint_callback] )\n",
    "duration=datetime.now() - start\n",
    "print(\"training completed in time:\",duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41bf5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "filename=r\"E:\\CHANDRU\\chand proj\\MOUD\\audioFiles\\134_makeup_3.wav\"\n",
    "audio, sr = librosa.load(filepath) \n",
    "mfccs_features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "print(mfccs_scaled_features.shape)\n",
    "predicted_label=model.predict(mfccs_scaled_features)\n",
    "predicted_label = np.argmax(predicted_label)\n",
    "prediction_class = le.inverse_transform([predicted_label])\n",
    "print(prediction_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285d112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
